"""
AI Idea Lab Pro - å‰µé€ çš„ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¢ãƒ—ãƒª
è¤‡æ•°ã®AIãŒã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ç™ºå±•ãƒ»ã‚¨ãƒ³ãƒãƒ³ã‚¹ã—ã€æœ€é«˜ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ç”Ÿã¿å‡ºã—ã¾ã™
"""
import streamlit as st
import time
from openai import OpenAI
import anthropic
import google.generativeai as genai

from config import (
    OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY,
    OPENAI_MODELS, ANTHROPIC_MODELS, GOOGLE_MODELS, ALL_MODELS,
    NO_TEMPERATURE_MODELS, SYSTEM_PROMPT, FACILITATOR_PROMPT,
    get_avatar, check_api_keys
)

# --- ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(
    page_title="AI Idea Lab Pro",
    page_icon="ğŸ’¡",
    layout="wide"
)

st.title("ğŸ’¡ AI Idea Lab Pro")
st.markdown("è¤‡æ•°ã®AIãŒã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ **ç™ºå±•ãƒ»ã‚¨ãƒ³ãƒãƒ³ã‚¹** ã—ã€æœ€é«˜ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ç”Ÿã¿å‡ºã—ã¾ã™")

# --- APIã‚­ãƒ¼çŠ¶æ³ã®ç¢ºèª ---
api_status = check_api_keys()

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
with st.sidebar:
    st.header("ğŸ”‘ API Keys çŠ¶æ³")
    
    # è¨­å®šçŠ¶æ³ã‚’è¡¨ç¤º
    for provider, is_set in api_status.items():
        if is_set:
            st.success(f"âœ… {provider.upper()}")
        else:
            st.error(f"âŒ {provider.upper()}")
    
    if not any(api_status.values()):
        st.warning("âš ï¸ .envãƒ•ã‚¡ã‚¤ãƒ«ã«APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        st.code("cp .env.example .env\n# .envã‚’ç·¨é›†ã—ã¦APIã‚­ãƒ¼ã‚’è¿½åŠ ", language="bash")
    
    st.markdown("---")
    st.header("ğŸ¤ ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚¿ãƒ¼é¸æŠ")
    st.caption("è­°è«–ã«å‚åŠ ã™ã‚‹AIã‚’é¸æŠï¼ˆè¤‡æ•°å¯ï¼‰")
    
    # ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆã‚«ãƒ†ã‚´ãƒªåˆ¥ï¼‰- APIã‚­ãƒ¼ãŒã‚ã‚‹ã‚‚ã®ã ã‘æœ‰åŠ¹åŒ–
    st.subheader("OpenAI", divider="green")
    selected_openai = st.multiselect(
        "OpenAIãƒ¢ãƒ‡ãƒ«",
        list(OPENAI_MODELS.keys()),
        default=["GPT-4o"] if api_status["openai"] else [],
        disabled=not api_status["openai"],
        label_visibility="collapsed"
    )
    
    st.subheader("Anthropic", divider="violet")
    selected_anthropic = st.multiselect(
        "Anthropicãƒ¢ãƒ‡ãƒ«",
        list(ANTHROPIC_MODELS.keys()),
        default=["Claude Sonnet 4"] if api_status["anthropic"] else [],
        disabled=not api_status["anthropic"],
        label_visibility="collapsed"
    )
    
    st.subheader("Google", divider="blue")
    selected_google = st.multiselect(
        "Googleãƒ¢ãƒ‡ãƒ«",
        list(GOOGLE_MODELS.keys()),
        default=["Gemini 2.5 Flash"] if api_status["google"] else [],
        disabled=not api_status["google"],
        label_visibility="collapsed"
    )
    
    # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ä¸€è¦§
    selected_models = selected_openai + selected_anthropic + selected_google
    
    st.markdown("---")
    st.header("ğŸ¯ ãƒ•ã‚¡ã‚·ãƒªãƒ†ãƒ¼ã‚¿ãƒ¼")
    
    # ãƒ•ã‚¡ã‚·ãƒªãƒ†ãƒ¼ã‚¿ãƒ¼å€™è£œï¼ˆé¸æŠã•ã‚Œã¦ã„ãªã„ãƒ¢ãƒ‡ãƒ« & APIã‚­ãƒ¼ãŒã‚ã‚‹ã‚‚ã®ï¼‰
    available_facilitators = []
    for m, (provider, _) in ALL_MODELS.items():
        if m not in selected_models and api_status.get(provider, False):
            available_facilitators.append(m)
    
    if available_facilitators:
        facilitator = st.selectbox(
            "ã¾ã¨ã‚å½¹ï¼ˆè­°è«–ã«ã¯å‚åŠ ã—ã¾ã›ã‚“ï¼‰",
            available_facilitators,
            index=0
        )
    else:
        st.warning("âš ï¸ ãƒ•ã‚¡ã‚·ãƒªãƒ†ãƒ¼ã‚¿ãƒ¼ç”¨ã«1ã¤ä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«ã‚’æ®‹ã—ã¦ãã ã•ã„")
        facilitator = None
    
    st.markdown("---")
    st.header("âš™ï¸ è¨­å®š")
    rounds = st.slider("ãƒ©ã‚¦ãƒ³ãƒ‰æ•°", 1, 10, 2)
    sleep_time = st.slider("ç”Ÿæˆé–“éš”ï¼ˆç§’ï¼‰", 0, 5, 1)
    
    # é¸æŠçŠ¶æ³ã®è¡¨ç¤º
    st.markdown("---")
    if selected_models:
        st.success(f"ğŸ¤ å‚åŠ è€…: {len(selected_models)}ä½“")
        for m in selected_models:
            st.write(f"  {get_avatar(m)} {m}")
    if facilitator:
        st.info(f"ğŸ¯ ãƒ•ã‚¡ã‚·ãƒªãƒ†ãƒ¼ã‚¿ãƒ¼: {get_avatar(facilitator)} {facilitator}")


# --- ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ– ---
@st.cache_resource
def init_clients():
    """APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰"""
    clients = {"openai": None, "anthropic": None, "google": None}
    
    if OPENAI_API_KEY:
        try:
            clients["openai"] = OpenAI(api_key=OPENAI_API_KEY)
        except Exception as e:
            st.error(f"OpenAIåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    if ANTHROPIC_API_KEY:
        try:
            clients["anthropic"] = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
        except Exception as e:
            st.error(f"AnthropicåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    if GOOGLE_API_KEY:
        try:
            genai.configure(api_key=GOOGLE_API_KEY)
            clients["google"] = True
        except Exception as e:
            st.error(f"GoogleåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    return clients


# --- AIå‘¼ã³å‡ºã—é–¢æ•° ---
def ask_ai(model_name: str, clients: dict, history_text: str, is_first: bool = False, topic: str = "") -> str:
    """æŒ‡å®šã•ã‚ŒãŸAIã«ç™ºè¨€ã•ã›ã‚‹"""
    provider, model_id = ALL_MODELS[model_name]
    
    if is_first:
        prompt = f"ãƒ†ãƒ¼ãƒ: {topic}\n\nã“ã®ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦ã€æœ€åˆã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚ãƒ¯ã‚¯ãƒ¯ã‚¯ã™ã‚‹ã‚ˆã†ãªã€å¯èƒ½æ€§ã‚’æ„Ÿã˜ã‚‹ææ¡ˆã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚"
    else:
        prompt = f"ã“ã‚Œã¾ã§ã®å¯¾è©±:\n{history_text}\n\nå‰ã®ç™ºè¨€è€…ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’å—ã‘ã¦ã€ã•ã‚‰ã«ç™ºå±•ã•ã›ã¦ãã ã•ã„ã€‚ã€ŒYes, Andã€ã®ç²¾ç¥ã§ã€ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ã‚¨ãƒ³ãƒãƒ³ã‚¹ã—ã¦ãã ã•ã„ã€‚"
    
    try:
        if provider == "openai":
            if not clients["openai"]:
                return "âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
            
            params = {
                "model": model_id,
                "messages": [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ]
            }
            if model_id not in NO_TEMPERATURE_MODELS:
                params["temperature"] = 0.9
            
            response = clients["openai"].chat.completions.create(**params)
            return response.choices[0].message.content
        
        elif provider == "anthropic":
            if not clients["anthropic"]:
                return "âŒ Anthropic APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
            
            response = clients["anthropic"].messages.create(
                model=model_id,
                max_tokens=1500,
                temperature=0.9,
                system=SYSTEM_PROMPT,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text
        
        elif provider == "google":
            if not clients["google"]:
                return "âŒ Google APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
            
            model = genai.GenerativeModel(model_id)
            full_prompt = f"{SYSTEM_PROMPT}\n\n{prompt}"
            response = model.generate_content(full_prompt)
            return response.text
            
    except Exception as e:
        return f"âŒ Error ({model_name}): {e}"


# --- ãƒ•ã‚¡ã‚·ãƒªãƒ†ãƒ¼ã‚¿ãƒ¼é–¢æ•° ---
def facilitate(facilitator_name: str, clients: dict, topic: str, full_log: str, collaborators: list) -> str:
    """å¯¾è©±ã‚’çµ±åˆã—ã€æœ€çµ‚ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ã¾ã¨ã‚ã‚‹"""
    provider, model_id = ALL_MODELS[facilitator_name]
    
    collab_list = "\n".join([f"- **{c}**: (ã“ã®AIãŒåŠ ãˆãŸç‹¬è‡ªã®è¦–ç‚¹ãƒ»ä¾¡å€¤ã‚’2-3è¡Œã§)" for c in collaborators])
    
    facilitator_prompt = FACILITATOR_PROMPT.format(
        topic=topic,
        collaborator_list=collab_list
    )
    full_prompt = f"{facilitator_prompt}\n\n--- å¯¾è©±ãƒ­ã‚° ---\n{full_log}"
    
    try:
        if provider == "openai":
            if not clients["openai"]:
                return "âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
            
            params = {
                "model": model_id,
                "messages": [
                    {"role": "system", "content": "ã‚ãªãŸã¯å‰µé€ çš„ãªå¯¾è©±ã®ãƒ•ã‚¡ã‚·ãƒªãƒ†ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚"},
                    {"role": "user", "content": full_prompt}
                ]
            }
            if model_id not in NO_TEMPERATURE_MODELS:
                params["temperature"] = 0.5
            
            response = clients["openai"].chat.completions.create(**params)
            return response.choices[0].message.content
        
        elif provider == "anthropic":
            if not clients["anthropic"]:
                return "âŒ Anthropic APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
            
            response = clients["anthropic"].messages.create(
                model=model_id,
                max_tokens=2500,
                temperature=0.5,
                system="ã‚ãªãŸã¯å‰µé€ çš„ãªå¯¾è©±ã®ãƒ•ã‚¡ã‚·ãƒªãƒ†ãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚",
                messages=[{"role": "user", "content": full_prompt}]
            )
            return response.content[0].text
        
        elif provider == "google":
            if not clients["google"]:
                return "âŒ Google APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
            
            model = genai.GenerativeModel(model_id)
            response = model.generate_content(full_prompt)
            return response.text
            
    except Exception as e:
        return f"âŒ ãƒ•ã‚¡ã‚·ãƒªãƒ†ãƒ¼ã‚¿ãƒ¼ã‚¨ãƒ©ãƒ¼ ({facilitator_name}): {e}"


# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
topic = st.text_input(
    "ğŸ’­ ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ç™ºå±•ã•ã›ãŸã„ãƒ†ãƒ¼ãƒã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", 
    "åœ°æ–¹ã®éç–åŒ–å•é¡Œã‚’è§£æ±ºã™ã‚‹é©æ–°çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ"
)

# ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
can_start = True
error_messages = []

if len(selected_models) < 2:
    error_messages.append("è­°è«–ã«ã¯2ä½“ä»¥ä¸Šã®AIã‚’é¸æŠã—ã¦ãã ã•ã„")
    can_start = False

if not facilitator:
    error_messages.append("ãƒ•ã‚¡ã‚·ãƒªãƒ†ãƒ¼ã‚¿ãƒ¼ã‚’é¸æŠã—ã¦ãã ã•ã„")
    can_start = False

if not any(api_status.values()):
    error_messages.append(".envãƒ•ã‚¡ã‚¤ãƒ«ã«APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„")
    can_start = False

for msg in error_messages:
    st.warning(f"âš ï¸ {msg}")

if st.button("ğŸš€ ã‚¢ã‚¤ãƒ‡ã‚¢ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹", disabled=not can_start, type="primary"):
    clients = init_clients()
    
    history_log = []
    chat_container = st.container()
    
    with chat_container:
        st.info(f"ğŸ’­ ãƒ†ãƒ¼ãƒ: {topic}")
        
        # å‚åŠ è€…è¡¨ç¤º
        participants_str = " & ".join([f"**{get_avatar(m)} {m}**" for m in selected_models])
        st.markdown(f"ğŸ¤ {participants_str}")
        st.markdown(f"ğŸ¯ ãƒ•ã‚¡ã‚·ãƒªãƒ†ãƒ¼ã‚¿ãƒ¼: **{get_avatar(facilitator)} {facilitator}**")
        st.markdown("---")

        # === å‰µé€ çš„å¯¾è©±ãƒ•ã‚§ãƒ¼ã‚º ===
        for i in range(rounds):
            st.markdown(f"### ğŸ’« ãƒ©ã‚¦ãƒ³ãƒ‰ {i+1}")
            
            for j, model in enumerate(selected_models):
                with st.chat_message("assistant", avatar=get_avatar(model)):
                    if i == 0 and j == 0:
                        st.write(f"**{model}** ğŸ’¡ æœ€åˆã®ã‚¢ã‚¤ãƒ‡ã‚¢...")
                    else:
                        st.write(f"**{model}** âœ¨ ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ç™ºå±•...")
                    
                    if i == 0 and j == 0:
                        msg = ask_ai(model, clients, "", is_first=True, topic=topic)
                    else:
                        context_text = "\n\n".join(history_log[-6:])
                        msg = ask_ai(model, clients, context_text)
                    
                    st.write(msg)
                
                history_log.append(f"[{model}]: {msg}")
                time.sleep(sleep_time)
        
        st.success("ğŸ‰ å¯¾è©±ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œäº†ï¼ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’çµ±åˆã—ã¾ã™...")
        st.markdown("---")
        
        # === ãƒ•ã‚¡ã‚·ãƒªãƒ†ãƒ¼ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚º ===
        st.markdown("## ğŸ¯ ã‚¢ã‚¤ãƒ‡ã‚¢çµ±åˆ")
        
        with st.spinner(f"ğŸ¯ {facilitator} ãŒã‚¢ã‚¤ãƒ‡ã‚¢ã‚’çµ±åˆä¸­..."):
            full_log = "\n\n".join(history_log)
            conclusion = facilitate(facilitator, clients, topic, full_log, selected_models)
        
        with st.chat_message("assistant", avatar=get_avatar(facilitator)):
            st.markdown(f"### ğŸ¯ ãƒ•ã‚¡ã‚·ãƒªãƒ†ãƒ¼ã‚¿ãƒ¼: {facilitator}")
            st.markdown(conclusion)
        
        st.markdown("---")
        st.balloons()
        
        # ãƒ­ã‚°ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        participants_list = ", ".join(selected_models)
        full_log_with_conclusion = (
            f"ãƒ†ãƒ¼ãƒ: {topic}\n"
            f"ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚¿ãƒ¼: {participants_list}\n"
            f"ãƒ•ã‚¡ã‚·ãƒªãƒ†ãƒ¼ã‚¿ãƒ¼: {facilitator}\n\n"
            f"{'='*50}\nğŸ’¬ å¯¾è©±ãƒ­ã‚°\n{'='*50}\n\n"
            f"{full_log}\n\n"
            f"{'='*50}\nğŸ¯ çµ±åˆã•ã‚ŒãŸã‚¢ã‚¤ãƒ‡ã‚¢ ({facilitator})\n{'='*50}\n\n"
            f"{conclusion}"
        )
        
        col1, col2 = st.columns(2)
        with col1:
            st.download_button(
                "ğŸ“œ å¯¾è©±ãƒ­ã‚°ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", 
                full_log, 
                file_name="idea_session_log.txt"
            )
        with col2:
            st.download_button(
                "ğŸ“‹ å®Œå…¨ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", 
                full_log_with_conclusion, 
                file_name="idea_session_full_report.txt"
            )
